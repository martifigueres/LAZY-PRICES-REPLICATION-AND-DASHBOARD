{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188bebc7-c03c-481a-ab70-d1377348f705",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing zip files: 100%|██████████| 10/10 [13:20<00:00, 80.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Extracted filings into 'sample_10k_data3'.\n",
      "✅ Metadata saved to 'extracted_metadata.csv'.\n",
      "📦 Creating archive 'sample_10k_data3.zip'...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(foldername, filename)\n\u001b[1;32m    108\u001b[0m             arcname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrelpath(file_path, output_root)\n\u001b[0;32m--> 109\u001b[0m             zipf\u001b[38;5;241m.\u001b[39mwrite(file_path, arcname)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Archive created: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/zipfile.py:1815\u001b[0m, in \u001b[0;36mZipFile.write\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     zinfo\u001b[38;5;241m.\u001b[39m_compresslevel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompresslevel\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(zinfo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[0;32m-> 1815\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(src, dest, \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/shutil.py:200\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m fdst_write(buf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/zipfile.py:1178\u001b[0m, in \u001b[0;36m_ZipWriteFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc \u001b[38;5;241m=\u001b[39m crc32(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compressor:\n\u001b[0;32m-> 1178\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compressor\u001b[38;5;241m.\u001b[39mcompress(data)\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#10_Ks only... about 6 minutes with filedate metadata\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Setup ---\n",
    "zip_folder = \".\"  # current folder\n",
    "output_root = \"sample_10k_data3\"\n",
    "sp500_file = \"sp500_companies.csv\"\n",
    "\n",
    "# Download S&P500 company list if not already downloaded\n",
    "if not os.path.exists(sp500_file):\n",
    "    url = 'https://en.wikipedia.org/w/index.php?title=List_of_S%26P_500_companies&oldid=1130173030'\n",
    "    pd.read_html(url)[0].to_csv(sp500_file, index=False)\n",
    "\n",
    "firms = pd.read_csv(sp500_file)\n",
    "\n",
    "# Grab set of S&P 500 tickers (not CIKs yet, but placeholder for future upgrades)\n",
    "tickers_sp500 = set(firms['Symbol'].str.upper())\n",
    "\n",
    "# --- Parameters ---\n",
    "cik_files = defaultdict(list)\n",
    "metadata = []  # 🆕 collect metadata here\n",
    "\n",
    "# Create output directory if needed\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Helper function to extract CIK from filename\n",
    "def extract_cik_from_member(member):\n",
    "    parts = member.split(\"edgar_data_\")\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    after_edgar = parts[1]\n",
    "    cik_part = after_edgar.split(\"_\")[0]\n",
    "    return cik_part if cik_part.isdigit() else None\n",
    "\n",
    "# --- Start Processing ---\n",
    "zip_files = [f for f in os.listdir(zip_folder) if f.endswith(\".zip\")]\n",
    "\n",
    "with tqdm(total=len(zip_files), desc=\"Processing zip files\") as pbar:\n",
    "    for zip_filename in zip_files:\n",
    "        zip_path = os.path.join(zip_folder, zip_filename)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "            for member in zipf.namelist():\n",
    "                if not member.endswith(\".txt\"):\n",
    "                    continue\n",
    "                if \"10-K\" not in member.upper():\n",
    "                    continue\n",
    "                if \"edgar_data_\" not in member:\n",
    "                    continue\n",
    "\n",
    "                # Extract CIK\n",
    "                cik = extract_cik_from_member(member)\n",
    "                if cik is None:\n",
    "                    continue\n",
    "\n",
    "                # Get year and quarter\n",
    "                path_parts = os.path.normpath(member).split(os.sep)\n",
    "                if len(path_parts) >= 3:\n",
    "                    year, quarter = path_parts[0], path_parts[1]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Destination directory\n",
    "                dest_dir = os.path.join(output_root, cik, year, quarter)\n",
    "                os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "                filename_only = os.path.basename(member)\n",
    "                dest_path = os.path.join(dest_dir, filename_only)\n",
    "\n",
    "                # ✅ Check if file already exists — if so, skip\n",
    "                if os.path.exists(dest_path):\n",
    "                    continue\n",
    "\n",
    "                # Stream copy file from zip to disk\n",
    "                with zipf.open(member) as src_file, open(dest_path, \"wb\") as dest_file:\n",
    "                    shutil.copyfileobj(src_file, dest_file)\n",
    "\n",
    "                cik_files[cik].append(dest_path)\n",
    "\n",
    "                # 🆕 Save metadata: CIK, Year, Quarter, Filing Date, Filename\n",
    "                filing_date = filename_only.split(\"_\")[0]\n",
    "                metadata.append((cik, year, quarter, filing_date, filename_only))\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"✅ Done! Extracted filings into '{output_root}'.\")\n",
    "\n",
    "# --- Save Metadata 🆕\n",
    "meta_df = pd.DataFrame(metadata, columns=[\"CIK\", \"Year\", \"Quarter\", \"Filing_Date\", \"Filename\"])\n",
    "meta_df.to_csv(\"extracted_metadata.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Metadata saved to 'extracted_metadata.csv'.\")\n",
    "\n",
    "# --- Zip the entire output folder ---\n",
    "zip_filename = f\"{output_root}.zip\"\n",
    "\n",
    "print(f\"📦 Creating archive '{zip_filename}'...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for foldername, subfolders, filenames in os.walk(output_root):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            arcname = os.path.relpath(file_path, output_root)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"✅ Archive created: '{zip_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d0294-b704-46c6-997e-f4aa67da2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas_datareader.famafrench as ff\n",
    "\n",
    "# --- Step 0: Setup ---\n",
    "text_folder = \"sample_10k_data3\"  # your extracted 10-Ks\n",
    "sp500_file = \"sp500_companies.csv\"  # your downloaded S&P500 info\n",
    "\n",
    "# Load S&P500 CIK list\n",
    "firms = pd.read_csv(sp500_file)\n",
    "firms['CIK'] = firms['CIK'].astype(str).str.zfill(10)\n",
    "sp500_ciks_padded = set(firms['CIK'])\n",
    "sp500_ciks_unpadded = set(cik.lstrip('0') for cik in sp500_ciks_padded)\n",
    "\n",
    "filing_metadata = []\n",
    "\n",
    "# --- Step 1: Build (Firm, Filing) Dataset ---\n",
    "\n",
    "print(\"🔍 Scanning filings...\")\n",
    "\n",
    "folder_list = os.listdir(text_folder)  # 📂 preload all folders once\n",
    "for cik_folder in tqdm(folder_list, desc=\"Scanning firms\"):\n",
    "    cik_path = os.path.join(text_folder, cik_folder)\n",
    "    if not os.path.isdir(cik_path):\n",
    "        continue\n",
    "\n",
    "    if cik_folder not in sp500_ciks_unpadded:\n",
    "        continue  # Only S&P500 firms\n",
    "\n",
    "    for year_folder in os.listdir(cik_path):\n",
    "        year_path = os.path.join(cik_path, year_folder)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        for quarter_folder in os.listdir(year_path):\n",
    "            quarter_path = os.path.join(year_path, quarter_folder)\n",
    "            if not os.path.isdir(quarter_path):\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(quarter_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    filing_date = file.split(\"_\")[0]  # YYYYMMDD\n",
    "                    filing_metadata.append((cik_folder, filing_date, os.path.join(quarter_path, file)))\n",
    "\n",
    "# Build filings dataframe\n",
    "filings_df = pd.DataFrame(filing_metadata, columns=[\"CIK\", \"FilingDate\", \"Filepath\"])\n",
    "filings_df[\"FilingDate\"] = pd.to_datetime(filings_df[\"FilingDate\"], format=\"%Y%m%d\")\n",
    "filings_df = filings_df.sort_values([\"CIK\", \"FilingDate\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Found {filings_df['CIK'].nunique()} S&P500 companies with filings.\")\n",
    "\n",
    "# --- Step 2: Preload all filings into memory ---\n",
    "\n",
    "print(\"📚 Preloading all filings into memory...\")\n",
    "\n",
    "file_contents = {}\n",
    "for filepath in tqdm(filings_df[\"Filepath\"], desc=\"Reading filings\"):\n",
    "    try:\n",
    "        with open(filepath, 'r', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "        file_contents[filepath] = text\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error reading {filepath}: {e}\")\n",
    "\n",
    "# --- Step 3: Precompute TF-IDF vectors ---\n",
    "\n",
    "print(\"⚡ Precomputing TF-IDF embeddings...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "all_texts = list(file_contents.values())\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "tfidf_vectors = {}\n",
    "for filepath, text in tqdm(file_contents.items(), desc=\"Vectorizing files\"):\n",
    "    try:\n",
    "        tfidf = vectorizer.transform([text])\n",
    "        tfidf_vectors[filepath] = tfidf\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"TF-IDF error for {filepath}: {e}\")\n",
    "\n",
    "# --- Step 4: Calculate Cosine Similarities (Parallel) ---\n",
    "\n",
    "print(\"⚡ Calculating cosine similarities in parallel...\")\n",
    "\n",
    "def calc_cosine_for_firm(cik, group):\n",
    "    records = []\n",
    "    if len(group) < 2:\n",
    "        return records\n",
    "\n",
    "    for i in range(1, len(group)):\n",
    "        prev_file = group.iloc[i-1][\"Filepath\"]\n",
    "        curr_file = group.iloc[i][\"Filepath\"]\n",
    "\n",
    "        if prev_file not in tfidf_vectors or curr_file not in tfidf_vectors:\n",
    "            continue\n",
    "\n",
    "        tfidf1 = tfidf_vectors[prev_file]\n",
    "        tfidf2 = tfidf_vectors[curr_file]\n",
    "\n",
    "        cos_sim = cosine_similarity(tfidf1, tfidf2)[0][0]\n",
    "        records.append((cik, group.iloc[i][\"FilingDate\"], cos_sim))\n",
    "    return records\n",
    "\n",
    "similarity_records = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for cik, group in filings_df.groupby(\"CIK\"):\n",
    "        futures.append(executor.submit(calc_cosine_for_firm, cik, group))\n",
    "\n",
    "    for future in tqdm(futures, desc=\"Firms for cosine\"):\n",
    "        result = future.result()\n",
    "        similarity_records.extend(result)\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_records, columns=[\"CIK\", \"FilingDate\", \"CosineSimilarity\"])\n",
    "similarity_df[\"FilingDate\"] = pd.to_datetime(similarity_df[\"FilingDate\"])\n",
    "\n",
    "# --- Step 5: Build (Firm, Month) returns dataset (Dummy) ---\n",
    "\n",
    "print(\"📈 Generating dummy returns...\")\n",
    "\n",
    "np.random.seed(0)\n",
    "firm_months = []\n",
    "\n",
    "for cik in tqdm(similarity_df[\"CIK\"].unique(), desc=\"Firms for returns\"):\n",
    "    for year in range(2010, 2025):\n",
    "        for month in range(1, 13):\n",
    "            date = pd.Timestamp(year=year, month=month, day=1)\n",
    "            ret = np.random.normal(0.01, 0.05)\n",
    "            firm_months.append((cik, date, ret))\n",
    "\n",
    "returns_df = pd.DataFrame(firm_months, columns=[\"CIK\", \"Month\", \"Return\"])\n",
    "\n",
    "# --- Step 6: Merge and Fill Quintiles ---\n",
    "\n",
    "print(\"🔗 Merging similarity into returns and creating quintiles...\")\n",
    "\n",
    "returns_df[\"Month\"] = pd.to_datetime(returns_df[\"Month\"])\n",
    "similarity_df[\"Year\"] = similarity_df[\"FilingDate\"].dt.year\n",
    "similarity_df[\"Quintile\"] = similarity_df.groupby(\"Year\")[\"CosineSimilarity\"].transform(\n",
    "    lambda x: pd.qcut(x, 5, labels=[1,2,3,4,5])\n",
    ")\n",
    "\n",
    "returns_df[\"Year\"] = returns_df[\"Month\"].dt.year\n",
    "returns_df = returns_df.merge(similarity_df[[\"CIK\", \"Year\", \"Quintile\"]], on=[\"CIK\", \"Year\"], how=\"left\")\n",
    "\n",
    "returns_df = returns_df.sort_values([\"CIK\", \"Month\"])\n",
    "returns_df[\"Quintile\"] = returns_df.groupby(\"CIK\")[\"Quintile\"].ffill(limit=5)\n",
    "\n",
    "# --- Step 7: Aggregate to Portfolio Returns ---\n",
    "\n",
    "print(\"📊 Building portfolio returns...\")\n",
    "\n",
    "returns_df = returns_df.dropna(subset=[\"Quintile\"])\n",
    "portfolio_returns = returns_df.groupby([\"Month\", \"Quintile\"])[\"Return\"].mean().unstack()\n",
    "\n",
    "portfolio_returns.columns = portfolio_returns.columns.astype(int)\n",
    "\n",
    "if 5 in portfolio_returns.columns and 1 in portfolio_returns.columns:\n",
    "    portfolio_returns[\"High-Low\"] = portfolio_returns[5] - portfolio_returns[1]\n",
    "    print(\"✅ High-Low portfolio computed successfully.\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Quintile 5 or 1 missing, skipping High-Low portfolio.\")\n",
    "\n",
    "# --- Step 8: Pull Fama-French Factors and Run Regressions ---\n",
    "\n",
    "print(\"📂 Downloading Fama-French factors...\")\n",
    "\n",
    "df_factors = ff.FamaFrenchReader('F-F_Research_Data_5_Factors_2x3', start='1900-01-01').read()[0]\n",
    "mom = ff.FamaFrenchReader('F-F_Momentum_Factor', start='1900-01-01').read()[0]\n",
    "mom.columns = ['Mom']\n",
    "df_factors = pd.merge(df_factors, mom, left_index=True, right_index=True)\n",
    "\n",
    "df_factors.index = pd.to_datetime(df_factors.index, format=\"%Y-%m\")\n",
    "reg_df = pd.merge(df_factors, portfolio_returns, left_index=True, right_index=True)\n",
    "\n",
    "# Regression Models\n",
    "factor_models = {\n",
    "    'r^e': '1',\n",
    "    'CAPM': 'Q(\"Mkt-RF\")',\n",
    "    'FF3': 'Q(\"Mkt-RF\") + SMB + HML',\n",
    "    'FF4': 'Q(\"Mkt-RF\") + SMB + HML + Mom',\n",
    "    'FF5': 'Q(\"Mkt-RF\") + SMB + HML + RMW + CMA',\n",
    "    'FF6': 'Q(\"Mkt-RF\") + SMB + HML + RMW + CMA + Mom'\n",
    "}\n",
    "\n",
    "portfolios = [1,2,3,4,5,\"High-Low\"]\n",
    "\n",
    "index = pd.MultiIndex.from_product([factor_models.keys(), ['alpha', 't-stat']], names=['Model', 'Metric'])\n",
    "results = pd.DataFrame(index=index, columns=portfolios, dtype=float)\n",
    "\n",
    "print(\"⚡ Running regressions...\")\n",
    "\n",
    "for portfolio in tqdm(portfolios, desc=\"Portfolios\"):\n",
    "    if portfolio not in reg_df.columns:\n",
    "        continue\n",
    "    for model_name, formula in factor_models.items():\n",
    "        reg = smf.ols(formula=f'Q({portfolio}) ~ {formula}', data=reg_df).fit()\n",
    "        alpha = reg.params['Intercept']\n",
    "        t_stat = reg.tvalues['Intercept']\n",
    "        results.at[(model_name, 'alpha'), portfolio] = alpha\n",
    "        results.at[(model_name, 't-stat'), portfolio] = t_stat\n",
    "\n",
    "print(\"✅ Done! Here are your results:\")\n",
    "display(results)\n",
    "\n",
    "# --- Save final table ---\n",
    "results.to_csv(\"final_portfolio_regressions.csv\")\n",
    "print(\"✅ Saved regression table to 'final_portfolio_regressions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91debe-c48b-4b0c-ba74-1b71f5739dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
